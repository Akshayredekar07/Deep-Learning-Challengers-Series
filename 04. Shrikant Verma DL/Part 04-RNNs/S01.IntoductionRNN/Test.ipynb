{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1POmzYEQZgZVqJO3F_cyNcd4QmoNE_mCv\n",
      "To: d:\\DEEP LEARNING\\04. Shrikant Verma DL\\Part 04-RNNs\\S01.IntoductionRNN\\bbc-news-data.csv\n",
      "\n",
      "  0%|          | 0.00/5.08M [00:00<?, ?B/s]\n",
      " 10%|█         | 524k/5.08M [00:00<00:02, 1.61MB/s]\n",
      " 21%|██        | 1.05M/5.08M [00:00<00:02, 1.87MB/s]\n",
      " 31%|███       | 1.57M/5.08M [00:00<00:01, 1.98MB/s]\n",
      " 41%|████▏     | 2.10M/5.08M [00:01<00:01, 2.09MB/s]\n",
      " 52%|█████▏    | 2.62M/5.08M [00:01<00:00, 2.56MB/s]\n",
      " 72%|███████▏  | 3.67M/5.08M [00:01<00:00, 3.75MB/s]\n",
      " 93%|█████████▎| 4.72M/5.08M [00:01<00:00, 4.62MB/s]\n",
      "100%|██████████| 5.08M/5.08M [00:01<00:00, 3.19MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !gdown 1POmzYEQZgZVqJO3F_cyNcd4QmoNE_mCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\n",
    "#     'bbc-news-data.csv',\n",
    "#     skiprows=1,                  # Skip the first row\n",
    "#     nrows=100,                   # Read only the first 100 rows\n",
    "#     names=['ID', 'Category', 'Text', 'Date'],  # Custom column names\n",
    "#     dtype={'ID': int, 'Category': str},  # Define column data types\n",
    "#     na_values=['?', 'NA'],       # Treat '?' and 'NA' as missing values\n",
    "#     parse_dates=['Date'],        # Parse the 'Date' column as datetime\n",
    "#     encoding='utf-8'             # Use UTF-8 encoding\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "...                                                 ...  \n",
       "2220   BT is introducing two initiatives to help bea...  \n",
       "2221   Computer users across the world continue to i...  \n",
       "2222   A new European directive could put software w...  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "2224   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-news-data.csv', encoding='utf-8', delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 4)\n",
      "(2225, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "...                                                 ...  \n",
       "2220   BT is introducing two initiatives to help bea...  \n",
       "2221   Computer users across the world continue to i...  \n",
       "2222   A new European directive could put software w...  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "2224   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the empty sub-title\n",
    "\"\"\"\n",
    "This script performs the following operations on a DataFrame `df`:\n",
    "1. Prints the initial shape of the DataFrame.\n",
    "2. Drops rows where either the 'title' or 'content' columns have null values.\n",
    "3. Resets the index of the DataFrame after dropping rows.\n",
    "4. Prints the shape of the DataFrame after dropping rows.\n",
    "5. Displays the resulting DataFrame.\n",
    "\"\"\"\n",
    "print(df.shape)\n",
    "\n",
    "df = df[((~df.title.isnull()) & (~df.content.isnull()))].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df.category.value_counts().index, df.category.value_counts().values)\n",
    "plt.title(\"Category distribution\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Federer joins all-time greats\n",
      "Category: sport\n"
     ]
    }
   ],
   "source": [
    "ind=1807\n",
    "print(f'Title: {df.title[ind]}')\n",
    "print(f'Category: {df.category[ind]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why MLP would not perform well for this problem?\n",
    "\n",
    "Before moving to MLP, let's understand what is temporal/Sequential data.\n",
    "\n",
    "**Temporal component:** Temporal means related to time. As we speak or write we do it in a sequence over time and it may not make sense when we change the order. Such type of data is called sequential/temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron\n",
    "\n",
    "Let's first build a MultiLayer Perceptron to train a FeedForward NeuralNetwork to classify the given sentence into any of the 5 categories.\n",
    "\n",
    "For the sake of understanding let's look at one example\n",
    "\n",
    "**Example sentence:**\n",
    "\n",
    "Title: \"Federer joins all time greats\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the example can be thought of as an individual time step in the sequence.\n",
    "```plaintext\n",
    "t-4 t-3 ... ... t\n",
    "Federer joins all time greats\n",
    "Xt-4 Xt-3 ... ... Xt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" alt=\"Image\" style=\"width:50%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is missing in this architecture?\n",
    "\n",
    "* **All inputs at once:**\n",
    "\n",
    "    A simple Feed Forward NN type of architecture assumes that all the inputs(all words from the example) come from a single time steps t. But in reality we speak/write one word after the other so there is mechanism of word order this architecture.\n",
    "\n",
    "* **Inputs of varying length:**\n",
    "\n",
    "    A MLP architecture can handle only a fixed-size input. In the example \"Federer joins all time greats\" the input neurons required are 5. If we have another input \"Nadal won the Australian open this year\", this example requires 7 input neurons.\n",
    "\n",
    "### Can we modify this architecture a bit to solve the above said problems?\n",
    "\n",
    "* **Pass each word at each timestep**\n",
    "\n",
    "    In the modified MLP architecture we will reuse the same set of weights and pass each words at a timestep from t-n to t. By doing this we are able to pass each word sequentially to the model instead of all at once.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image1.png\" alt=\"Image\" style=\"width:50%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there something still missing? Yes!!!\n",
    "\n",
    "Language makes sense only when spoken in an order and the current word depends on the previous words spoken/written\n",
    "\n",
    "Let's look at a simple example where you will have to fill in the last word\n",
    "\n",
    "**Fill the last word**\n",
    "\n",
    "Scenario 1: _______\n",
    "\n",
    "Scenario 2: Sachin Tendulkar is a _______\n",
    "\n",
    "Answer: Sachin Tendulkar is a **cricketer**\n",
    "\n",
    "In Scenario 1, without having access to the previous words it is literally impossible to fill the blank as cricketer. However in scenario 2 it's easy to fill because we know the context of what are the previous words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **No connection from previous input words**\n",
    "\n",
    "    As we pass one word at a time step t, the final output \"sports\" is not only a function of the last word \"greats\" passed at time t. But it also depends on the other words \"Federer joins all time\" as well which are passed from time steps tn-6 tn-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Each word is isolated from the other**\n",
    "\n",
    "    Even after passing data sequentially by reusing the MLP architecture again and again for each time step, each of the input is still isolated and independent from each other\n",
    "\n",
    "**What is missing in this modified architecture?**\n",
    "\n",
    "* **No Temporal component:**\n",
    "\n",
    "    The temporal component is missing here. Since the MLP model assumes the entire input from X1 to x₁ comes all at once instead of one after the other at each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's summarize the problems\n",
    "\n",
    "* All input passed at once\n",
    "* Can't handle varying length inputs\n",
    "* No Temporal connections in the modified MLP\n",
    "\n",
    "### Can we overcome all these?\n",
    "\n",
    "Yes!!! RNN comes for the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need is an architecture where the current input at time t is processed along with all the prior inputs from t-n to t-1.\n",
    "\n",
    "To achieve this we will introduce a memory(hidden) state h. This memory state will carry the information from the previous time step to the current time step in a recursive manner. So the information is processed together at the current time step.\n",
    "\n",
    "Because of this recurrence relation the output is going to be a function of input at the current time step along with the inputs from previous time steps\n",
    "\n",
    "$$ O_t = f(x_t, h_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"image2.png\" alt=\"Image\" style=\"width:20%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use them?\n",
    "\n",
    "When we are dealing with sequential data. In other terms when we have a temporal component associated to it and the data makes sense only when followed a sequence.\n",
    "\n",
    "**Sequence:** Federer joins all time greats\n",
    "\n",
    "**No sequence:** all Federer greats time joins\n",
    "\n",
    "**Example Data:** Time series data(stock price, Weather data), Genomic data, Language texts.\n",
    "\n",
    "A noteable difference of RNN is how we use the hiddent state (hₜ₋₁) from the previous time step input(Xₜ₋₁) as one of the input for the current time step(t) along with the input (Xₜ)\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "* **All inputs at once:**\n",
    "\n",
    "    A simple Feed Forward NN type of architecture assumes that all the inputs(all words from the example) come from a single time steps t. But in reality we speak/write one word after the other so there is mechanism of word order in this architecture.\n",
    "\n",
    "* **Inputs of varying length:**\n",
    "\n",
    "    A simple MLP architecture can handle only a fixed-size input. But RNNs can accommodate variable size input since we are reusing the same RNN unit again and again. In the example \"Federer joins all time greats\" which is 5 word long can also be passed to the same RNN unit with another example \"Nadal won the Australian open this year\" which is 7 words long.\n",
    "\n",
    "* **Weight sharing:**\n",
    "\n",
    "    RNN is a single unit cells used at every time step. Due to the nature of reusability the weights are shared across all the inputs. This also helps in reduced computation compared to a MLP architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image3.png\" alt=\"Image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are the different types of RNN?**\n",
    "\n",
    "**Types of input & output**\n",
    "\n",
    "There are couple of variants of RNN based on the number of inputs and the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"image4.png\" alt=\"Image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One to One:** T<sub>x</sub>=1, T<sub>y</sub>=1\n",
    "\n",
    "**Example:** Normal Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many to One:** T<sub>x</sub>>1, T<sub>y</sub>=1\n",
    "\n",
    "**Example:** Sentiment classification\n",
    "\n",
    "Sentiment classification is a task of passing a phrase or sentence about a movie review to the model and output if the sentiment is positive or negative.\n",
    "\n",
    "**Many to Many:** T<sub>x</sub>>1, T<sub>y</sub>>1, T<sub>x</sub>!=T<sub>y</sub>\n",
    "\n",
    "**Example:** Language translation\n",
    "\n",
    "Language translation is the task of inputting text to a model in one language and predict the same text in another language of interest(Google translator).\n",
    "\n",
    "**Many to Many:** T<sub>x</sub>>1, T<sub>y</sub>>1, T<sub>x</sub>=T<sub>y</sub>\n",
    "\n",
    "**Example:** Named Entity Recognition\n",
    "\n",
    "Name entity recognition is a task of automatically identifying keywords in the text. The keywords could be the name of the person, name of a company, city, phone number, email etc from the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"image5.png\" alt=\"Image\" style=\"width:50%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xt:**\n",
    "\n",
    "Xₜ is the input at time step t. In our example above \"Federer joins all time greats\" each word is passed at a time.\n",
    "Note: When we say we pass each word to the network we first convert each words to a vector representation of defined length and then pass it to the RNN unit.\n",
    "\n",
    "**ht-1**\n",
    "\n",
    "Hidden state from the previous time step which is vector representation of the information from previous time steps\n",
    "Note: When we are passing the first input the hidden states are initialized to all zeros\n",
    "\n",
    "**tanh:**\n",
    "\n",
    "The input from the current time step t and the hidden state from the previous time step t-1 are combined (addition) together and a non-linear activation function is applied. In case of Vanilla RNN it is the tanh\n",
    "\n",
    "**ht**\n",
    "\n",
    "Hiddent state from the current time step t. This state is updated at everytime step which is a function of the input at current time step t and the previous hidden state hₜ₋₁\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oₜ** \n",
    "\n",
    "Oₜ is the output of RNN. It can be single or multi output based on the use case. Also based on the use cases we can get output at everytime step or only at the last time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But do we pass the words as is to the RNN cell?**\n",
    "\n",
    "No we don't, We all know machines can't understand text but only numbers\n",
    "\n",
    "Then, How the do we convert the text to numbers? Let's understand with an example.\n",
    "\n",
    "There are 2 ways this can be done\n",
    "\n",
    "1. Using pre-trained word embeddings like Word2Vec, Glove, ELMO etc.\n",
    "2. Learning the embeddings while training the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example sentence 1:** \"the cat sat on the same wall the rat sat\"\n",
    "\n",
    "**Example sentence 2:** \"the cat likes milk\"\n",
    "\n",
    "**Method 1**\n",
    "\n",
    "* For the above 2 example sentences we convert each word to vectors using the pretrained embedding and then pass them to the RNN cell.\n",
    "* We can set the trainable method to False if we don't want to fine-tune these embeddings while training.\n",
    "* We can choose the dimension of the word embedding for these pretrained word embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**\n",
    "\n",
    "* First we create a vocabulary of all unique words in the dataset\n",
    "* For the above sentences the vocabulary size is 9 and we assign index to each word.\n",
    "\n",
    "{cat:0, likes:1, milk:2, on:3, rat:4, same:5, sat:6, the:7, wall:8}\n",
    "\n",
    "* There is a embedding layer of said dimension before passing it to the RNN cell\n",
    "* The word embeddings are assigned to each word indexes i.e 0 to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The word embeddings are randomly initialized in the embedding layer for the given dimension and is learnt during the training process by setting trainable=True\n",
    "\n",
    "Let's say we choose embedding dimension as 3 then embedding vector for each word would look like\n",
    "\n",
    "    cat(0) = [0.2, 0.64, 0.4]\n",
    "    likes(1) = [0.7, 0.34, 0.4]\n",
    "    milk(2) = [0.6, 0.5, 0.23]\n",
    "    on(3) = [0.1, 0.2, 0.04]\n",
    "    rat(4) = [0.94, 0.8, 0.9]\n",
    "    same(5) = [0.45, 0.23, 0.26]\n",
    "    sat(6) = [0.78, 0.9, 0.5]\n",
    "    the(7) = [0.25, 0.16, 0.2]\n",
    "    wall(8) = [0.11, 0.93, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The embedding dimensions could be of any size. The most commonly used dimensions are 100 to 300.\n",
    "\n",
    "## What is being learnt?\n",
    "\n",
    "* Similar to the MLP architecture we have weight matrices in RNN which are learnt during the training\n",
    "\n",
    "    * **W<sub>hx</sub>:** The weight matrix at the input\n",
    "    * **W<sub>hh</sub>:** The weight matrix at the hidden state\n",
    "    * **W<sub>hy</sub>:** The weight matrix at the output\n",
    "\n",
    "* **Note:** The same set of weights will be applied at each time steps during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image6.png\" alt=\"Image\" style=\"width:100%;\">\n",
    "<img src=\"image7.png\" alt=\"Image\" style=\"width:100%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Forward Propagation works in RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image8.png\" alt=\"Image\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Forward Propagation\n",
    "\n",
    "For each word in the input:\n",
    "\n",
    "1. Multiply previous hidden state with weight matrix W\n",
    "2. Multiply input word with weight matrix U\n",
    "3. Add 1. and 2. and bias\n",
    "4. Apply the Non-Linear Activation Function (tanh)\n",
    "    * i. A copy of 4. is multiplied with weight matrix V\n",
    "    * ii. Add bias to 4i. and apply the non-Linear Activation function(based on usecase)\n",
    "    * iii. Calculate Loss (based on usecase)\n",
    "5. A copy of 4. is passed as hidden state to the next time step\n",
    "\n",
    "**The above steps can be mathematically generalized as**\n",
    "\n",
    "Z<sub>ht</sub> = UX<sub>t</sub> + Wh<sub>t-1</sub> + b<sub>h</sub>\n",
    "\n",
    "h<sub>t</sub> = σ<sub>h</sub>(Z<sub>ht</sub>)\n",
    "\n",
    "Z<sub>yt</sub> = V<sup>T</sup>h<sub>t</sub> + b<sub>y</sub>\n",
    "\n",
    "O<sub>t</sub> = σ<sub>y</sub>(Z<sub>yt</sub>)\n",
    "\n",
    "L = L(O<sub>t</sub>, Y<sub>t</sub>)\n",
    "\n",
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image9.png\" alt=\"Image\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usual steps involved in updating parameter weights:\n",
    "* Calculate the gradients of the loss with respect to the parameters\n",
    "* Multiply it with the Learning rate\n",
    "* Update the new weights\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$ V = V - \\alpha \\frac{\\partial L}{\\partial V} $$\n",
    "\n",
    "$$ W = W - \\alpha \\frac{\\partial L}{\\partial W} $$\n",
    "\n",
    "$$ U = U - \\alpha \\frac{\\partial L}{\\partial U} $$\n",
    "\n",
    "$\\alpha$ = Learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image11.png\" alt=\"Image\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation Through Time (BPTT) vs. Regular Backpropagation**\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "1. **Shared Weights:**\n",
    "   - In traditional Multi-Layer Perceptrons (MLPs), each layer has its own distinct set of weights.\n",
    "   - In Recurrent Neural Networks (RNNs), the same weight matrices are reused at every time step.\n",
    "\n",
    "2. **Gradient Calculation:**\n",
    "   - In MLPs, backpropagation calculates gradients layer by layer, updating weights based on the errors at each layer.\n",
    "   - In BPTT, gradients are calculated across all time steps, accumulating the error signals to make a single update to the shared weights.\n",
    "\n",
    "**In Essence:**\n",
    "\n",
    "BPTT extends the concept of backpropagation to handle the sequential nature of RNNs. It accounts for the shared weights and calculates gradients by \"unfolding\" the RNN over time, effectively treating it as a very deep neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "**What is a Loss function and why we need?**\n",
    "\n",
    "* The Loss function helps to determine how good or bad the model is performing comparing it with the actual output.\n",
    "* These are many loss functions based on the problem we are solving and the most commonly used loss metrics are\n",
    "    * Regression: RMSE, MSE, SSE\n",
    "    * Binary classification: Binary cross-entrophy loss\n",
    "    * Multiclass classification: Multiclass cross-entrophy loss\n",
    "\n",
    "**What Loss function should we use?**\n",
    "\n",
    "Since we are solving a Multiclass classification problem, Multiclass cross-entrophy loss is the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_{t}=-Y \\log(O_{t})$$\n",
    "\n",
    "### **Optimizing with respect to V**\n",
    "\n",
    "* As a first step we need to calculate the gradients of the loss Lₜ at time t with respect to the weight matrix V\n",
    "* As we go through the path we see that Oₜ depends on Zₜ to reach V and will make use of the chain rule to calculate the gradients. \n",
    "\n",
    "$$\\frac{\\partial{L_t}}{\\partial{V}} = \\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{V}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Update the weight matrix V**\n",
    "\n",
    "$$V = V - α ∂Lₜ/∂V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with respect to W\n",
    "\n",
    "* As a first step we need to calculate the gradients of the loss Lₜ at time t with respect to the weight matrix W\n",
    "* As we go through the path we see that hₜ depends on W and hₜ₋₁ directly. But hₜ₋₁ indirectly depends on W.\n",
    "* When we look at hₜ₋₁, it directly depends on W and hₜ₋₂. But hₜ₋₂ indirectly depends on W.\n",
    "* This keeps chaining due to the dependencies on time and we can calculate the gradients with respect to each time step using the chain rule in calculus\n",
    "* A very important thing to note here is the weight matrix W is common across all the time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**General Formula:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{W}} = \\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{W}}\n",
    "$$\n",
    "\n",
    "**Gradients of loss with respect to W at different t:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial h_{t}}{\\partial W} = \\frac{\\partial h_{t}}{\\partial W} + \\frac{\\partial h_{t}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t-1}}{\\partial W} = \\frac{\\partial h_{t-1}}{\\partial W} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} * \\frac{\\partial h_{t-2}}{\\partial W} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t-2}}{\\partial W} = \\frac{\\partial h_{t-2}}{\\partial W} + \\frac{\\partial h_{t-2}}{\\partial h_{t-3}} * \\frac{\\partial h_{t-3}}{\\partial W} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t-3}}{\\partial W} = \\frac{\\partial h_{t-3}}{\\partial W} + \\frac{\\partial h_{t-3}}{\\partial h_{t-4}} * \\frac{\\partial h_{t-4}}{\\partial W} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t-4}}{\\partial W} = \\frac{\\partial h_{t-4}}{\\partial W} + \\frac{\\partial h_{t-4}}{\\partial h_{t-5}} * \\frac{\\partial h_{t-5}}{\\partial W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial O_t} * \\frac{\\partial O_t}{\\partial Z_t} * \\frac{\\partial Z_t}{\\partial h_t} * \\left( \\frac{\\partial h_t}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-2}} * \\frac{\\partial h_{t-2}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-3}} * \\frac{\\partial h_{t-3}}{\\partial h_{t-2}} * \\frac{\\partial h_{t-2}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-4}} * \\frac{\\partial h_{t-4}}{\\partial h_{t-3}} * \\frac{\\partial h_{t-3}}{\\partial h_{t-2}} * \\frac{\\partial h_{t-2}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-5}} * \\frac{\\partial h_{t-5}}{\\partial h_{t-4}} * \\frac{\\partial h_{t-4}}{\\partial h_{t-3}} * \\frac{\\partial h_{t-3}}{\\partial h_{t-2}} * \\frac{\\partial h_{t-2}}{\\partial h_{t-1}} * \\frac{\\partial h_{t-1}}{\\partial W} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Expanding it___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{W}} = \\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{h_{t-1}}} * \\frac{\\partial{h_{t-1}}}{\\partial{W}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{h_{t-1}}} * \\frac{\\partial{h_{t-1}}}{\\partial{h_{t-2}}} * \\frac{\\partial{h_{t-2}}}{\\partial{W}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{h_{t-1}}} * \\frac{\\partial{h_{t-1}}}{\\partial{h_{t-2}}} * \\frac{\\partial{h_{t-2}}}{\\partial{h_{t-3}}} * \\frac{\\partial{h_{t-3}}}{\\partial{W}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{h_{t-1}}} * \\frac{\\partial{h_{t-1}}}{\\partial{h_{t-2}}} * \\frac{\\partial{h_{t-2}}}{\\partial{h_{t-3}}} * \\frac{\\partial{h_{t-3}}}{\\partial{h_{t-4}}} * \\frac{\\partial{h_{t-4}}}{\\partial{W}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\frac{\\partial{h_t}}{\\partial{h_{t-1}}} * \\frac{\\partial{h_{t-1}}}{\\partial{h_{t-2}}} * \\frac{\\partial{h_{t-2}}}{\\partial{h_{t-3}}} * \\frac{\\partial{h_{t-3}}}{\\partial{h_{t-4}}} * \\frac{\\partial{h_{t-4}}}{\\partial{h_{t-5}}} * \\frac{\\partial{h_{t-5}}}{\\partial{W}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let generalize this to a single formula\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{W}} = \\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\sum_{r=1}^{t} \\frac{\\partial{h_t}}{\\partial{h_{t-r}}} * \\frac{\\partial{h_{t-r}}}{\\partial{W}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Update the weight matrix W**\n",
    "\n",
    "$$W = W - α ∂Lₜ/∂W$$\n",
    "\n",
    "#### Optimizing with respect to U\n",
    "\n",
    "Taking the derivative of U is similar to how we calculated the gradients with respect to W. It also requires taking sequential derivatives with respect to hₜ. So we can rewrite the same formula we derived and replace W with U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{L_t}}{\\partial{U}} = \\frac{\\partial{L_t}}{\\partial{O_t}} * \\frac{\\partial{O_t}}{\\partial{Z_t}} * \\frac{\\partial{Z_t}}{\\partial{h_t}} * \\sum_{r=1}^{t} \\frac{\\partial{h_t}}{\\partial{h_{t-r}}} * \\frac{\\partial{h_{t-r}}}{\\partial{U}}\n",
    "$$\n",
    "\n",
    "#### Update the weight matrix U\n",
    "\n",
    "$$U = U - α ∂Lₜ/∂U$$\n",
    "\n",
    "#### What if we had a multi-output problem? MultiLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image12.png\" alt=\"Image\" style=\"width:70%;\">\n",
    "<img src=\"image13.png\" alt=\"Image\" style=\"width:70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L = \\sum_{r=1}^{t} L_r$$\n",
    "\n",
    "* Can we use information from the future as well?\n",
    "\n",
    "Yes we can and it's called **Bidirectional RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image14.png\" alt=\"Image\" style=\"width:70%;\">\n",
    "<img src=\"image15.png\" alt=\"Image\" style=\"width:70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario 1: Sachin Tendular was ______\n",
    "\n",
    "Scenario 2: Sachin Tendular was ______ of Rajya Sabha\n",
    "\n",
    "Answer: Sachin Tendular was **MP** of Rajya Sabha\n",
    "\n",
    "In the scenario 2, it's easier to predict the word \"MP\" if we had access to input from both the left and the right of the current word instead of only the words from the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | UniDirectional RNN | BiDirectional RNN |\n",
    "|---|---|---|\n",
    "| Information flow | Left to Right | Left to Right & Right to Left |\n",
    "| Output depends | h<sub>t-1</sub>, X<sub>t</sub> | h<sub>t-1</sub>, X<sub>t</sub>, h<sub>t+1</sub> |\n",
    "| Weights | W, U, V | W, U, V, W', U', V' |\n",
    "\n",
    "**▼ Code Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, InputLayer, RNN, SimpleRNN, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "\n",
    "    # Lower the words in the sentence\n",
    "    cleaned = text.lower()\n",
    "\n",
    "    # Replace the full stop with a full stop and space\n",
    "    cleaned = cleaned.replace(\".\", \". \")\n",
    "\n",
    "    # Remove the stop words\n",
    "    tokens = [word for word in cleaned.split() if not word in stop_words]\n",
    "    # tokens = []  # Initialize an empty list to store tokens\n",
    "    # for word in cleaned.split():\n",
    "    #     if word not in stop_words:\n",
    "    #         tokens.append(word)\n",
    "\n",
    "    # Remove the punctuations\n",
    "    tokens = [tok.translate(str.maketrans('', '', string.punctuation)) for tok in tokens]\n",
    "\n",
    "    # Joining the tokens back to form the sentence\n",
    "    cleaned = \" \".join(tokens)\n",
    "\n",
    "    # Remove any extra spaces\n",
    "    cleaned = cleaned.strip()\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Input Example**:\n",
    "```python\n",
    "text = \"Hello, world! This is a sample text. Let's clean it.\"\n",
    "stop_words = {\"is\", \"a\", \"it\", \"this\"}  # Define some stop words\n",
    "```\n",
    "\n",
    "### **Dry Run Steps:**\n",
    "\n",
    "#### 1. **Lowercase the Words:**\n",
    "```python\n",
    "cleaned = text.lower()\n",
    "# cleaned = \"hello, world! this is a sample text. let's clean it.\"\n",
    "```\n",
    "\n",
    "#### 2. **Replace Full Stop with Full Stop and Space:**\n",
    "```python\n",
    "cleaned = cleaned.replace(\".\", \". \")\n",
    "# cleaned = \"hello, world! this is a sample text.  let's clean it.\"\n",
    "```\n",
    "\n",
    "#### 3. **Remove Stop Words:**\n",
    "- Split the string into words: `cleaned.split()`  \n",
    "  → `['hello,', 'world!', 'this', 'is', 'a', 'sample', 'text.', \"let's\", 'clean', 'it.']`\n",
    "\n",
    "- Filter out the stop words (`stop_words = {\"is\", \"a\", \"it\", \"this\"}`):\n",
    "```python\n",
    "tokens = [word for word in cleaned.split() if not word in stop_words]\n",
    "# tokens = ['hello,', 'world!', 'sample', 'text.', \"let's\", 'clean', 'it.']\n",
    "```\n",
    "\n",
    "#### 4. **Remove Punctuation:**\n",
    "- Remove punctuation using `str.maketrans()`:\n",
    "  ```python\n",
    "  tokens = [tok.translate(str.maketrans('', '', string.punctuation)) for tok in tokens]\n",
    "  # tokens = ['hello', 'world', 'sample', 'text', 'lets', 'clean', 'it']\n",
    "  ```\n",
    "\n",
    "#### 5. **Join Tokens Back into a Sentence:**\n",
    "```python\n",
    "cleaned = \" \".join(tokens)\n",
    "# cleaned = \"hello world sample text lets clean it\"\n",
    "```\n",
    "\n",
    "#### 6. **Remove Extra Spaces:**\n",
    "- Remove any leading or trailing spaces (though none exist here):\n",
    "```python\n",
    "cleaned = cleaned.strip()\n",
    "# cleaned = \"hello world sample text lets clean it\"\n",
    "```\n",
    "\n",
    "### **Final Output:**\n",
    "```python\n",
    "\"hello world sample text lets clean it\"\n",
    "```\n",
    "\n",
    "### **Explanation of Each Step:**\n",
    "1. **Lowercasing** ensures case uniformity.\n",
    "2. **Adding a space after full stops** ensures proper tokenization later.\n",
    "3. **Removing stop words** reduces noise in the data.\n",
    "4. **Removing punctuation** ensures tokens are only words.\n",
    "5. **Joining tokens** reconstructs a cleaned sentence.\n",
    "6. **Stripping extra spaces** polishes the result for clean output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:00<00:00, 7693.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "for index, data in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    df.loc[index, 'title'] = data_cleaning(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='title'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(df['title'].str.split(\" \").str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code**:\n",
    "```python\n",
    "np.random.seed(100)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(df['title'], \n",
    "                                                 df['category'], \n",
    "                                                 test_size=0.2, \n",
    "                                                 random_state=100)\n",
    "\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)\n",
    "train_Y = train_Y.reset_index(drop=True)\n",
    "test_Y = test_Y.reset_index(drop=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Assume Sample DataFrame `df`**:\n",
    "```plaintext\n",
    "   title                                            category\n",
    "0  \"This is a short title\"                               news\n",
    "1  \"This title is slightly longer than the previous one\"    news\n",
    "2  \"A very long title indeed with many words\"              sports\n",
    "3  \"Breaking news: Something happened\"                    news\n",
    "4  \"Sports update: Big game tomorrow\"                     sports\n",
    "5  \"Another news headline for testing\"                    news\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Dry Run**:\n",
    "\n",
    "#### Step 1: Set the random seed\n",
    "```python\n",
    "np.random.seed(100)\n",
    "```\n",
    "- This ensures the random splitting is reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Split the data into training and testing sets\n",
    "```python\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(df['title'], \n",
    "                                                    df['category'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=100)\n",
    "```\n",
    "- **test_size=0.2**: 20% of the data is allocated to the testing set, and 80% to the training set.\n",
    "- **random_state=100**: Ensures consistent splitting.\n",
    "\n",
    "The split might result in:\n",
    "\n",
    "**Training Data**:\n",
    "```plaintext\n",
    "train_X: \n",
    "1  \"This title is slightly longer than the previous one\"\n",
    "5  \"Another news headline for testing\"\n",
    "4  \"Sports update: Big game tomorrow\"\n",
    "2  \"A very long title indeed with many words\"\n",
    "\n",
    "train_Y:\n",
    "1  \"news\"\n",
    "5  \"news\"\n",
    "4  \"sports\"\n",
    "2  \"sports\"\n",
    "```\n",
    "\n",
    "**Testing Data**:\n",
    "```plaintext\n",
    "test_X: \n",
    "3  \"Breaking news: Something happened\"\n",
    "0  \"This is a short title\"\n",
    "\n",
    "test_Y:\n",
    "3  \"news\"\n",
    "0  \"news\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Reset the indices\n",
    "```python\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)\n",
    "train_Y = train_Y.reset_index(drop=True)\n",
    "test_Y = test_Y.reset_index(drop=True)\n",
    "```\n",
    "- The indices of the resulting training and testing sets are reset to start from 0.\n",
    "\n",
    "After resetting indices:\n",
    "\n",
    "**Training Data**:\n",
    "```plaintext\n",
    "train_X:\n",
    "0  \"This title is slightly longer than the previous one\"\n",
    "1  \"Another news headline for testing\"\n",
    "2  \"Sports update: Big game tomorrow\"\n",
    "3  \"A very long title indeed with many words\"\n",
    "\n",
    "train_Y:\n",
    "0  \"news\"\n",
    "1  \"news\"\n",
    "2  \"sports\"\n",
    "3  \"sports\"\n",
    "```\n",
    "\n",
    "**Testing Data**:\n",
    "```plaintext\n",
    "test_X:\n",
    "0  \"Breaking news: Something happened\"\n",
    "1  \"This is a short title\"\n",
    "\n",
    "test_Y:\n",
    "0  \"news\"\n",
    "1  \"news\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Result**:\n",
    "The data is split into training and testing sets, with indices reset. The training set contains 4 samples (80%), and the testing set contains 2 samples (20%). This ensures a clean structure for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 7\n",
      "Total classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum sentence length\n",
    "max_sentence_len = df['title'].str.split(\" \").str.len().max()\n",
    "total_classes = df.category.nunique()\n",
    "\n",
    "print(f\"Maximum sequence length: {max_sentence_len}\")\n",
    "print(f\"Total classes: {total_classes}\")\n",
    "\n",
    "# Output:\n",
    "# Maximum sequence length: 7\n",
    "# Total classes: 5\n",
    "\n",
    "# Splitting the data to train and test\n",
    "np.random.seed(100)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(df['title'], \n",
    "                                                 df['category'], \n",
    "                                                 test_size=0.2, \n",
    "                                                 random_state=100)\n",
    "\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)\n",
    "train_Y = train_Y.reset_index(drop=True)\n",
    "test_Y = test_Y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encode the labels\n",
    "train_Y = pd.get_dummies(train_Y).values\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "\n",
    "# Get validation labels\n",
    "validation = test_Y.argmax(axis=1) \n",
    "\n",
    "# Tokenize the input text and pad them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "train_Y = pd.get_dummies(train_Y).values\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "```\n",
    "\n",
    "- `pd.get_dummies()` converts each unique category in the data into a binary column.\n",
    "\n",
    "#### Processing `train_Y`:\n",
    "- Unique categories: `['sports', 'news', 'entertainment']`.\n",
    "- Each category is assigned a binary vector:\n",
    "  - `sports`: `[1, 0, 0]`\n",
    "  - `news`: `[0, 1, 0]`\n",
    "  - `entertainment`: `[0, 0, 1]`\n",
    "- Transformed `train_Y`:\n",
    "```python\n",
    "array([[1, 0, 0],   # sports\n",
    "       [0, 1, 0],   # news\n",
    "       [0, 0, 1],   # entertainment\n",
    "       [1, 0, 0],   # sports\n",
    "       [0, 1, 0]])  # news\n",
    "```\n",
    "\n",
    "#### Processing `test_Y`:\n",
    "- Unique categories are the same: `['sports', 'news', 'entertainment']`.\n",
    "- Transformed `test_Y`:\n",
    "```python\n",
    "array([[0, 1, 0],   # news\n",
    "       [1, 0, 0],   # sports\n",
    "       [0, 0, 1],   # entertainment\n",
    "       [1, 0, 0]])  # sports\n",
    "```\n",
    "### Step 2: Extracting Validation Labels\n",
    "\n",
    "#### Code:\n",
    "```python\n",
    "validation = test_Y.argmax(axis=1)\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "- `argmax(axis=1)` finds the index of the maximum value in each row.\n",
    "- Each index corresponds to the original category:\n",
    "  - `[0, 1, 0]` → `1` (news)\n",
    "  - `[1, 0, 0]` → `0` (sports)\n",
    "  - `[0, 0, 1]` → `2` (entertainment)\n",
    "\n",
    "#### Result:\n",
    "```python\n",
    "validation = array([1, 0, 2, 0])  # [news, sports, entertainment, sports]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Dry Run Summary:\n",
    "\n",
    "1. **One-Hot Encoded `train_Y`**:\n",
    "```python\n",
    "array([[1, 0, 0],  # sports\n",
    "       [0, 1, 0],  # news\n",
    "       [0, 0, 1],  # entertainment\n",
    "       [1, 0, 0],  # sports\n",
    "       [0, 1, 0]]) # news\n",
    "```\n",
    "\n",
    "2. **One-Hot Encoded `test_Y`**:\n",
    "```python\n",
    "array([[0, 1, 0],  # news\n",
    "       [1, 0, 0],  # sports\n",
    "       [0, 0, 1],  # entertainment\n",
    "       [1, 0, 0]]) # sports\n",
    "```\n",
    "\n",
    "3. **Validation Labels (`validation`)**:\n",
    "```python\n",
    "array([1, 0, 2, 0])  # [news, sports, entertainment, sports]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall text vocab size: 3339\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_pad(inp_text, max_len, tok):\n",
    "    text_seq = tok.texts_to_sequences(inp_text)\n",
    "    text_seq = pad_sequences(text_seq, maxlen=max_len, padding='post')\n",
    "    return text_seq\n",
    "\n",
    "text_tok = Tokenizer()\n",
    "text_tok.fit_on_texts(train_X)\n",
    "\n",
    "train_text_X = tokenize_and_pad(inp_text=train_X, max_len=max_sentence_len, tok=text_tok)\n",
    "test_text_X = tokenize_and_pad(inp_text=test_X, max_len=max_sentence_len, tok=text_tok)\n",
    "\n",
    "vocab_size = len(text_tok.word_index) + 1\n",
    "\n",
    "print(\"Overall text vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to tokenize and pad input text\n",
    "def tokenize_and_pad(input_text, max_length, tokenizer):\n",
    "    # Convert text into sequences of numbers based on the tokenizer\n",
    "    tokenized_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "    # Pad the sequences to the specified max_length\n",
    "    padded_sequences = pad_sequences(tokenized_sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "# Step 1: Initialize the tokenizer\n",
    "text_tokenizer = Tokenizer()\n",
    "\n",
    "# Step 2: Fit the tokenizer on training data\n",
    "text_tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "# Step 3: Tokenize and pad training data\n",
    "train_padded_sequences = tokenize_and_pad(\n",
    "    input_text=train_X, \n",
    "    max_length=max_sentence_len, \n",
    "    tokenizer=text_tokenizer\n",
    ")\n",
    "\n",
    "# Step 4: Tokenize and pad test data\n",
    "test_padded_sequences = tokenize_and_pad(\n",
    "    input_text=test_X, \n",
    "    max_length=max_sentence_len, \n",
    "    tokenizer=text_tokenizer\n",
    ")\n",
    "\n",
    "# Step 5: Calculate the vocabulary size\n",
    "# Adding 1 because index 0 is reserved for padding\n",
    "vocabulary_size = len(text_tokenizer.word_index) + 1\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Overall text vocabulary size:\", vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry Run:\n",
    "\n",
    "#### Given Inputs:\n",
    "- `train_X` = [\"The quick brown fox jumps over the lazy dog\", \"A journey of a thousand miles begins with a single step\"]\n",
    "- `test_X` = [\"The lazy dog lies down\", \"A thousand miles away\"]\n",
    "- `max_sentence_len` = 10\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Step 1: Initialize Tokenizer**  \n",
    "   - `text_tokenizer = Tokenizer()`  \n",
    "   Initializes a Tokenizer object to handle the text processing.\n",
    "\n",
    "2. **Step 2: Fit Tokenizer on Training Data**  \n",
    "   - `text_tokenizer.fit_on_texts(train_X)`  \n",
    "   This builds the vocabulary from the `train_X` data. After fitting, `text_tokenizer.word_index` will look something like:\n",
    "   ```python\n",
    "   {\n",
    "       'the': 1,\n",
    "       'a': 2,\n",
    "       'of': 3,\n",
    "       'a': 4,\n",
    "       'thousand': 5,\n",
    "       'and': 6,\n",
    "       'miles': 7,\n",
    "       'quick': 8,\n",
    "       ...\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **Step 3: Tokenize and Pad Training Data**  \n",
    "   - `train_padded_sequences = tokenize_and_pad(...)`  \n",
    "   Convert each sentence into a sequence of integers (tokenized) and pad them to 10 tokens:\n",
    "   ```python\n",
    "   # Tokenized and padded sequences might look like this:\n",
    "   [[1, 8, 9, 4, 10, 11, 3, 12, 13, 14],  # \"The quick brown fox ...\"\n",
    "    [2, 15, 3, 7, 16, 4, 17, 18, 19, 20]]  # \"A journey of a thousand ...\"\n",
    "   ```\n",
    "\n",
    "4. **Step 4: Tokenize and Pad Test Data**  \n",
    "   - `test_padded_sequences = tokenize_and_pad(...)`  \n",
    "   Similar tokenization and padding applied to the test data:\n",
    "   ```python\n",
    "   [[1, 13, 14, 6, 21, 22, 23, 24, 0, 0],  # \"The lazy dog lies down\"\n",
    "    [2, 5, 7, 8, 25, 0, 0, 0, 0, 0]]  # \"A thousand miles away\"\n",
    "   ```\n",
    "\n",
    "5. **Step 5: Vocabulary Size Calculation**  \n",
    "   - `vocabulary_size = len(text_tokenizer.word_index) + 1`  \n",
    "   The vocabulary size is the total unique words in the `train_X` data + 1 for the padding token:\n",
    "   ```python\n",
    "   vocabulary_size = 18\n",
    "   ```\n",
    "\n",
    "#### Output:\n",
    "```plaintext\n",
    "Overall text vocabulary size: 18\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent dimension: Dimension of the weight matrix U, V, W\n",
    "latent_dim = 50\n",
    "\n",
    "# Embedding dimension: Dimension of the word embeddings at the embedding layer\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the RNN model architecture\n",
    "\n",
    "    * The embedding layer with 100 dimension\n",
    "    * A single Vanilla RNN unit with 50 dimensions\n",
    "    * A final output layer with 5 units (5 classes) with the softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed=56\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, trainable=True))\n",
    "model.add(SimpleRNN(latent_dim, recurrent_dropout=0.2, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(total_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "seed = 56\n",
    "tf.random.set_seed(seed)  # Sets the seed for TensorFlow's random number generator\n",
    "np.random.seed(seed)     # Sets the seed for NumPy's random number generator\n",
    "```\n",
    "\n",
    "### Why Use a Seed?\n",
    "1. **Reproducibility**:  \n",
    "   When you train a machine learning or deep learning model, random processes (like initializing weights, shuffling data, and dropout) are involved. Without a seed, the results will vary slightly every time you run the code. By setting a seed, you ensure the same random numbers are used, leading to consistent results.\n",
    "\n",
    "2. **Debugging**:  \n",
    "   If a specific model behavior or issue arises, using a seed allows you to reproduce the exact same conditions, making it easier to debug.\n",
    "\n",
    "3. **Fair Comparisons**:  \n",
    "   When comparing multiple models or parameter settings, a seed ensures that the randomness is consistent across experiments. This makes it easier to determine which model or configuration performs better under identical conditions.\n",
    "\n",
    "### Randomness in the Code:\n",
    "In the given model:\n",
    "1. **Embedding Layer Initialization**:  \n",
    "   The `Embedding` layer uses random initialization for the word embeddings.\n",
    "2. **SimpleRNN Layer Initialization**:  \n",
    "   The weights of the RNN are initialized randomly.\n",
    "3. **Dropout in RNN**:  \n",
    "   The `recurrent_dropout` parameter in the `SimpleRNN` layer involves random dropping of recurrent connections.\n",
    "\n",
    "Without a seed, these processes would differ each time the model is run, leading to different results.\n",
    "\n",
    "### Summary:\n",
    "Setting the seed ensures that:\n",
    "- The random initialization of weights in layers is consistent.\n",
    "- Any randomness in data preprocessing or augmentation is reproducible.\n",
    "- The behavior of stochastic operations (e.g., dropout) is consistent across runs.\n",
    "\n",
    "This is especially useful when sharing results or collaborating on a project to ensure everyone observes the same outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model training**\n",
    "\n",
    "* **Optimizer:** Adam\n",
    "* **Loss:** Categorical cross-entropy since it is a multiclass classification problem\n",
    "* **Early stopping:** Used to stop training if validation accuracy does not improve while training to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', \n",
    "                               mode='max', \n",
    "                               verbose=1, \n",
    "                               patience=5)\n",
    "\n",
    "model.fit(x=train_text_X, y=train_Y, \n",
    "          validation_data=(test_text_X, test_Y),\n",
    "          batch_size=64, \n",
    "          epochs=10, \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Model Compilation**\n",
    "```python\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "```\n",
    "- **`optimizer='Adam'`**: Specifies the Adam optimizer, which is widely used because it combines the benefits of both **Adagrad** and **RMSProp** optimizers. Adam adapts learning rates during training, making it effective for a variety of tasks.\n",
    "  \n",
    "- **`loss='categorical_crossentropy'`**: Specifies the loss function to be minimized. Since this is a classification problem with one-hot encoded labels, categorical cross-entropy is used. It measures the difference between the true labels and the predicted probabilities.\n",
    "\n",
    "- **`metrics=['acc']`**: Specifies accuracy as the evaluation metric during training and validation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Early Stopping Callback**\n",
    "```python\n",
    "early_stopping = EarlyStopping(monitor='val_acc', \n",
    "                               mode='max', \n",
    "                               verbose=1, \n",
    "                               patience=5)\n",
    "```\n",
    "- **`EarlyStopping`**: This is a callback that stops training when a monitored metric stops improving.\n",
    "  \n",
    "- **`monitor='val_acc'`**: Monitors the validation accuracy (`val_acc`).\n",
    "  \n",
    "- **`mode='max'`**: Looks for the maximum value of the monitored metric (validation accuracy). Training stops if the validation accuracy doesn’t improve.\n",
    "  \n",
    "- **`verbose=1`**: Enables detailed messages about when early stopping occurs.\n",
    "  \n",
    "- **`patience=5`**: Specifies how many epochs to wait before stopping if no improvement is observed. Here, training stops if validation accuracy doesn’t improve for 5 consecutive epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Model Training**\n",
    "```python\n",
    "model.fit(x=train_text_X, y=train_Y, \n",
    "          validation_data=(test_text_X, test_Y),\n",
    "          batch_size=64, \n",
    "          epochs=10, \n",
    "          callbacks=[early_stopping])\n",
    "```\n",
    "\n",
    "#### Parameters:\n",
    "- **`x=train_text_X, y=train_Y`**: Training data (`train_text_X`) and corresponding labels (`train_Y`).\n",
    "  \n",
    "- **`validation_data=(test_text_X, test_Y)`**: Validation data and labels used to evaluate the model’s performance after each epoch. This does not affect training but helps monitor overfitting.\n",
    "\n",
    "- **`batch_size=64`**: The number of samples processed before the model updates its weights. A batch size of 64 means the data is split into chunks of 64 samples for processing.\n",
    "\n",
    "- **`epochs=10`**: The maximum number of passes over the entire training dataset.\n",
    "\n",
    "- **`callbacks=[early_stopping]`**: Passes the early stopping callback, so training stops early if validation accuracy does not improve for 5 epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow:\n",
    "1. The model is trained on `train_text_X` and `train_Y`.\n",
    "2. After each epoch:\n",
    "   - The loss and accuracy are calculated on the training data.\n",
    "   - Validation loss and accuracy are calculated using `test_text_X` and `test_Y`.\n",
    "   - Early stopping checks if validation accuracy has improved.\n",
    "3. If validation accuracy does not improve for 5 epochs, training halts early to avoid overfitting or wasting resources.\n",
    "4. If the model runs for all 10 epochs, it stops after completing them.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use These Settings?\n",
    "1. **Adam Optimizer**: Efficient and works well for most tasks.\n",
    "2. **Categorical Cross-Entropy**: Appropriate for multi-class classification.\n",
    "3. **Early Stopping**: Prevents overfitting and reduces computation time by stopping when further training is unlikely to improve performance.\n",
    "4. **Validation Data**: Helps monitor the model's ability to generalize to unseen data.\n",
    "\n",
    "This setup ensures efficient training while avoiding overfitting or unnecessary computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "prediction = model.predict(test_text_X)\n",
    "prediction = prediction.argmax(axis=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(prediction, validation)}\") \n",
    "\n",
    "# Confusion matrix of the prediction and actual\n",
    "cm = confusion_matrix(validation, prediction) \n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Oranges\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Things to keep in mind**\n",
    "\n",
    "* RNN takes the current input at time t as well as the hidden state from the previous time stamp t-1.\n",
    "* There are 3 weight Matrices W, U, V from the input, hidden state and output and W, U, V, W', U', V' respectively interms of bi-directional RNN\n",
    "* BiDirectional RNN process information from left to right and also from riogh\n",
    "* The same RNN unit is reused for all the input so weights are shared across all time steps.\n",
    "* The gradients are calculated over time which is called the BPPT.\n",
    "* Backpropagation through time: what it does and how to do it- https://axon.cs.byu.edu/Dan/678/papers/Recurrent/Werbos.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
