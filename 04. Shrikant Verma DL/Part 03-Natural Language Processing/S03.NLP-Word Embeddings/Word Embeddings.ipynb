{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63d7200",
   "metadata": {},
   "source": [
    "\n",
    "### What Does *Semantic* Mean?\n",
    "\n",
    "**Semantics** refers to the **meaning** or **interpretation of words, phrases, and sentences** in a language.\n",
    "\n",
    "In the context of **NLP (Natural Language Processing)**:\n",
    "\n",
    "- **Semantic similarity** measures **how close the meanings of two words or texts are**, not just whether the words are the same.\n",
    "- Two different words (e.g., *good* and *great*) can have **high semantic similarity** if they mean similar things.\n",
    "- Conversely, words like *good* and *terrible* have **low semantic similarity** even if they appear in similar contexts.\n",
    "\n",
    "####  Example:\n",
    "- Lexical (word-based) view:  \n",
    "  *“good” ≠ “great”* (not the same word)\n",
    "- Semantic view:  \n",
    "  *“good” ≈ “great”* (very similar meaning)\n",
    "\n",
    "#### Semantic vectors capture:\n",
    "- Word meaning\n",
    "- Contextual usage\n",
    "- Relationships like synonymy (similar) and antonymy (opposite)\n",
    "\n",
    "\n",
    "## Notes on Document and Word Vector Representations\n",
    "\n",
    "### **Corpus and Word Representation**\n",
    "\n",
    "- A **corpus** is a collection of documents:\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  d_1 &\\rightarrow w_3\\ w_6\\ w_5\\ w_3\\ -\\ - \\\\\n",
    "  d_2 &\\rightarrow w_1\\ w_{12}\\ w_6\\ w_3\\ -\\ - \\\\\n",
    "  \\vdots \\\\\n",
    "  d_n &\\rightarrow \\text{(other sequences of words)}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "- Each document $ d_i $ is a sequence of **words** $ w_j $.\n",
    "- Each word $ w_i $ is mapped to a **vector representation** in $ \\mathbb{R}^d $, i.e., a **d-dimensional vector**.\n",
    "\n",
    "#### Interpretation:\n",
    "- This mapping can be learned through models like **Word2Vec, GloVe, or FastText**.\n",
    "- The goal is to encode semantic meaning in vectors.\n",
    "\n",
    "### **Word Embeddings and Semantic Representations**\n",
    "\n",
    "Let’s look at examples of how words are embedded as vectors and how we measure similarity:\n",
    "\n",
    "#### Example 1:\n",
    "- **good** → $ v_1 $\n",
    "- **great** → $ v_2 $\n",
    "\n",
    "Both are semantically similar. So,\n",
    "$$\n",
    "\\text{CosineSimilarity}(v_1, v_2) \\approx 1\n",
    "$$\n",
    "This means they point in nearly the same direction in vector space.\n",
    "\n",
    "#### Example 2:\n",
    "- **terrible** → $ v_3 $\n",
    "- Now compare:\n",
    "$$\n",
    "\\text{CosineSimilarity}(v_1, v_3) \\ll 1 \\quad \\text{(very low)}\n",
    "$$\n",
    "\n",
    "#### Insight:\n",
    "- High cosine similarity → semantically similar\n",
    "- Low cosine similarity → semantically different or opposite\n",
    "\n",
    "\n",
    "### **What Does Cosine Similarity Measure?**\n",
    "\n",
    "- Measures the **angle** between two vectors, not their magnitude.\n",
    "- Formula:\n",
    "  &&\n",
    "  \\text{CosineSimilarity}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\|v_2\\|}\n",
    "  &&\n",
    "- Value ranges from:\n",
    "  - **+1**: very similar\n",
    "  - **0**: orthogonal, unrelated\n",
    "  - **-1**: opposite meanings\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Concept               | Description |\n",
    "|-|-|\n",
    "| Document $ d_i $     | A sequence of word tokens |\n",
    "| Word $ w_j $         | Each token in the document |\n",
    "| Vector $ v \\in \\mathbb{R}^d $ | d-dimensional vector capturing meaning |\n",
    "| Cosine Similarity     | Measures semantic closeness between two words |\n",
    "| High Similarity       | Indicates semantic similarity (e.g., *good*, *great*) |\n",
    "| Low Similarity        | Indicates semantic difference (e.g., *good*, *terrible*) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f6fb6",
   "metadata": {},
   "source": [
    "Here's the content with all headings changed to h4 and bolded:\n",
    "\n",
    "#### **1. Corpus and Vocabulary**\n",
    "\n",
    "- Given a **corpus** of $ n $ documents:  \n",
    "  $$\n",
    "  \\mathcal{D} = \\{d_1, d_2, \\dots, d_n\\}\n",
    "  $$\n",
    "- Vocabulary extracted:  \n",
    "  $$\n",
    "  \\mathcal{V} = \\{w_1, w_2, \\dots, w_N\\}\n",
    "  $$\n",
    "  where $ N = |\\mathcal{V}| $ is the total number of unique words.\n",
    "\n",
    "#### **2. Document-Term Matrix $ A \\in \\mathbb{R}^{n \\times N} $**\n",
    "\n",
    "This is a basic way to represent text data numerically.\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "& w_1 & w_2 & \\dots & w_N \\\\\n",
    "d_1 & 1 & 2 & \\dots & \\cdots \\\\\n",
    "d_2 & 0 & 2 & \\dots & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "d_n & \\cdots & \\cdots & \\cdots & \\cdots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- **Rows** $ \\rightarrow $ Documents $ d_i $\n",
    "- **Columns** $ \\rightarrow $ Words $ w_j $\n",
    "- **Entries** $ A_{ij} $ = occurrence count of $ w_j $ in $ d_i $\n",
    "\n",
    "#### **3. Challenges with BoW Representation**\n",
    "\n",
    "- $ N $ is large $ \\rightarrow $ **High dimensionality**\n",
    "- Each document uses only a small subset of words $ \\rightarrow $ **Very sparse**\n",
    "- **No order information** is retained $ \\rightarrow $ **Loss of sequence/context**\n",
    "\n",
    "#### **4. Word Co-Occurrence Matrix $ \\widetilde{W}_{N \\times N} $**\n",
    "\n",
    "To model **word relationships**, a **word-word co-occurrence matrix** is used.\n",
    "\n",
    "$$\n",
    "\\widetilde{W} =\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\cdots & w_j & \\cdots \\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "w_i & \\cdots & 1 & \\cdots \\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\quad \\text{(Window-based counts)}\n",
    "$$\n",
    "\n",
    "- Rows and columns: Words in vocabulary\n",
    "- Entry $ \\widetilde{W}_{ij} $: Number of times word $ w_j $ appeared in the **context window** of word $ w_i $\n",
    "\n",
    "This matrix partially encodes **sequential context**, using a window (e.g., size = 5) around a central word.\n",
    "\n",
    "#### **5. Matrix Factorization of Document-Term Matrix**\n",
    "\n",
    "We approximate the document-term matrix $ A \\in \\mathbb{R}^{n \\times N} $ using **low-rank matrix factorization**:\n",
    "\n",
    "$$\n",
    "A \\approx B C^\\top\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ B \\in \\mathbb{R}^{n \\times d} $: document representations (dense)\n",
    "- $ C \\in \\mathbb{R}^{N \\times d} $: word representations (dense)\n",
    "- $ d \\ll N $: dimension of latent semantic space\n",
    "\n",
    "In expanded form:\n",
    "$$\n",
    "A_{n \\times N} \\approx B_{n \\times d} \\cdot C_{N \\times d}^\\top\n",
    "$$\n",
    "\n",
    "##### 5.1 **Interpretation of Factors**\n",
    "\n",
    "###### Document Matrix $ B $:\n",
    "$$\n",
    "B =\n",
    "\\begin{pmatrix}\n",
    "\\vdots \\\\\n",
    "\\rule{1cm}{0.15mm} \\leftarrow i \\\\\n",
    "\\vdots \\\\\n",
    "\\end{pmatrix}_{n \\times d}\n",
    "\\quad \\text{= vector representation of document } d_i\n",
    "$$\n",
    "\n",
    "###### Word Matrix $ C $:\n",
    "$$\n",
    "C =\n",
    "\\begin{pmatrix}\n",
    "\\vdots & \\uparrow \\\\\n",
    "\\rule{1cm}{0.15mm} & j \\\\\n",
    "\\vdots & \\downarrow \\\\\n",
    "\\end{pmatrix}_{N \\times d}\n",
    "\\quad \\text{= vector representation of word } w_j\n",
    "$$\n",
    "\n",
    "#### **6. Objective Function for Matrix Factorization**\n",
    "\n",
    "The aim is to minimize the reconstruction error over the **non-zero entries** of $ A $:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{\\substack{i,j \\\\ A_{ij} \\neq 0}} (A_{ij} - B_i^\\top C_j)^2 + \\text{regularization terms}\n",
    "$$\n",
    "\n",
    "- This objective penalizes the squared difference between actual and predicted co-occurrence\n",
    "- Can be extended to include L2 regularization on $ B $ and $ C $ for generalization\n",
    "\n",
    "#### **7. Key Insight**\n",
    "\n",
    "Matrix factorization achieves two key goals:\n",
    "- Compresses large sparse matrix $ A $ into lower-dimensional dense matrices $ B, C $\n",
    "- Captures **semantic similarity**: similar words and documents end up with nearby vectors in $ \\mathbb{R}^d $\n",
    "\n",
    "### From Corpus to Word Vectors — Skip-Gram with Negative Sampling\n",
    "\n",
    "#### **1. Problem Setup**\n",
    "\n",
    "We are given a **corpus**:\n",
    "$$\n",
    "\\mathcal{D} = [w_1, w_2, \\dots, w_T]\n",
    "$$\n",
    "where each $ w_t \\in \\mathcal{V} $, and $ \\mathcal{V} $ is a vocabulary of size $ N $.\n",
    "\n",
    "Our goal:  \n",
    "Map each word $ w_i \\in \\mathcal{V} $ to a **dense, low-dimensional** vector:\n",
    "$$\n",
    "w_i \\mapsto v_i \\in \\mathbb{R}^d \\quad \\text{(where } d \\ll N\\text{)}\n",
    "$$\n",
    "so that similar words have similar vectors.\n",
    "\n",
    "#### **2. Context and Co-Occurrence**\n",
    "\n",
    "We define **context** using a sliding window of size $ k $.  \n",
    "For a center word $ w_i $, context is:\n",
    "$$\n",
    "\\text{Context}(w_i) = \\{ w_{i-k}, ..., w_{i-1}, w_{i+1}, ..., w_{i+k} \\}\n",
    "$$\n",
    "\n",
    "From this, we generate **co-occurrence pairs**:\n",
    "$$\n",
    "(w_i, w_j) \\quad \\text{if } w_j \\in \\text{Context}(w_i)\n",
    "$$\n",
    "This gives us a binary co-occurrence matrix:\n",
    "$$\n",
    "C_{N \\times N} = \\begin{cases}\n",
    "1 & \\text{if } (w_i, w_j) \\text{ co-occur in context} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### **3. Neural Architecture (SGNS)**\n",
    "\n",
    "Each word is first represented as a **One-Hot Encoding (OHE)**:\n",
    "\n",
    "- For word $ w_j \\in \\mathcal{V} $,  \n",
    "$$\n",
    "\\text{OHE}(w_j) = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\leftarrow \\text{j-th position} \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix} \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "The architecture:\n",
    "\n",
    "1. **Input**: OHE vector of target word $ w_i $\n",
    "   $$\n",
    "   \\text{OHE}(w_i) \\rightarrow W_{\\text{in}} \\in \\mathbb{R}^{N \\times d}\n",
    "   $$\n",
    "   Produces:\n",
    "   $$\n",
    "   v_i = W_{\\text{in}}^\\top \\text{OHE}(w_i) \\in \\mathbb{R}^d\n",
    "   $$\n",
    "\n",
    "2. **Output**: OHE vector of context word $ w_j $\n",
    "   $$\n",
    "   \\text{OHE}(w_j) \\rightarrow W_{\\text{out}} \\in \\mathbb{R}^{N \\times d}\n",
    "   $$\n",
    "   Produces:\n",
    "   $$\n",
    "   v_j = W_{\\text{out}}^\\top \\text{OHE}(w_j) \\in \\mathbb{R}^d\n",
    "   $$\n",
    "\n",
    "Thus we get:\n",
    "$$\n",
    "v_i = \\text{embedding of center word}, \\quad v_j = \\text{embedding of context word}\n",
    "$$\n",
    "\n",
    "#### **4. Prediction**\n",
    "\n",
    "To predict whether $ w_j $ is a true context word for $ w_i $, compute:\n",
    "$$\n",
    "s_{ij} = v_i^\\top v_j \\quad \\text{(dot product)}\n",
    "$$\n",
    "and apply:\n",
    "$$\n",
    "\\hat{y}_{ij} = \\sigma(v_i^\\top v_j) = \\frac{1}{1 + e^{-v_i^\\top v_j}} \\in [0, 1]\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- If $ \\hat{y}_{ij} \\approx 1 $, model believes $ w_j $ is a likely context for $ w_i $\n",
    "- If $ \\hat{y}_{ij} \\approx 0 $, model believes it is not\n",
    "\n",
    "#### **5. Training Objective: Negative Sampling**\n",
    "\n",
    "Training uses:\n",
    "- **Positive samples**: actual context pairs $(w_i, w_j)$\n",
    "- **Negative samples**: randomly sampled $ w_k \\notin \\text{Context}(w_i) $\n",
    "\n",
    "For each center-context pair $ (w_i, w_j) $, optimize:\n",
    "$$\n",
    "\\mathcal{L}_{ij} = -\\log \\sigma(v_i^\\top v_j) - \\sum_{k=1}^K \\mathbb{E}_{w_k \\sim P_n(w)} \\left[ \\log \\sigma(-v_i^\\top v_k) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\sigma(\\cdot) $ is the sigmoid\n",
    "- $ K $ is number of negative samples\n",
    "- $ P_n(w) $ is the noise distribution (typically proportional to $ f(w)^{3/4} $)\n",
    "\n",
    "#### **6. Learning**\n",
    "\n",
    "The model is trained using SGD (or Adam). The learned matrices:\n",
    "- $ W_{\\text{in}} \\rightarrow $ word vectors (input embeddings)\n",
    "- $ W_{\\text{out}} \\rightarrow $ context vectors (output embeddings)\n",
    "\n",
    "You can use either or average them:\n",
    "$$\n",
    "v_w = \\frac{W_{\\text{in}}(w) + W_{\\text{out}}(w)}{2}\n",
    "$$\n",
    "\n",
    "#### **7. Outcome**\n",
    "\n",
    "After training:\n",
    "- Each word $ w $ gets a dense vector $ v_w \\in \\mathbb{R}^d $\n",
    "- Words that **co-occur in similar contexts** end up **closer** in embedding space\n",
    "- Enables:\n",
    "  - Analogy tasks (e.g., *king - man + woman ≈ queen*)\n",
    "  - Semantic clustering\n",
    "  - Downstream NLP applications\n",
    "\n",
    "- **2012–2013**:\n",
    "  - Word2Vec introduced by Mikolov et al.\n",
    "  - First model: **CBOW** — predict word from context\n",
    "  - Then: **Skip-Gram** — predict context from word\n",
    "  - Then: **SGNS** — Skip-Gram + Negative Sampling (computationally efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb3719",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d12f4f",
   "metadata": {},
   "source": [
    "#### With over 50,000 publications and research papers on the corona-virus family till date, it has become difficult to search across and get useful insights for medical practitioners.\n",
    "\n",
    "As a Data Scientist at Google, you are tasked with solving this problem with the help of Machine Learning.\n",
    "\n",
    "Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "\n",
    "How can we solve for this problem?\n",
    "\n",
    "* Can we match keywords from user queries that are present in abstract?\n",
    "* If we do keyword matching, will we be able to understand the user's intent? For e.g. 'origin' and 'discovery'\n",
    "* Should we consider the context of the words, then?\n",
    "\n",
    "Let us build a search engine using Word Embeddings\n",
    "\n",
    "Dataset \n",
    "\n",
    "* COVID-19 Open Research Dataset, consisting of all publications/research papers related to Covid-19.\n",
    "* Dataset contains Title, Abstract, Dol among other identifiers.\n",
    "* To download - [https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f4baa",
   "metadata": {},
   "source": [
    "### All remaing word in Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
