{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting our words in the corpus into vector of integers:\n",
      "[[1, 3, 4, 5, 1, 6], [1, 7, 8, 2, 1, 9], [1, 10, 11, 2, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [\n",
    "    'The cat sat on the mat',\n",
    "    'The dog ran in the park',\n",
    "    'The bird sang in the tree'\n",
    "]\n",
    "\n",
    "# Convert the corpus to a sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"After converting our words in the corpus into vector of integers:\")\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 10\n",
    "window_size = 2\n",
    "\n",
    "# Generate the context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert the contexts and targets to numpy arrays\n",
    "X = np.array(contexts)\n",
    "y = to_categorical(targets, num_classes=vocab_size)\n",
    "\n",
    "# Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2 * window_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings\n",
    "embedding_layer = model.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the embeddings\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(5, 5))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    x, y = reduced_embeddings[idx]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "plt.title(\"Word Embeddings Visualized\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9a030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences as numbers: [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 1, 9], [1, 10, 11, 8, 1, 12]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     y[i, target] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Set the target word's position to 1\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Step 3: Build the model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Layer 1: Convert words to vectors (embeddings)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39membedding_size, input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m window_size),\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Layer 2: Average the vectors of context words\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mreduce_mean(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Layer 3: Predict the target word\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     Dense(units\u001b[38;5;241m=\u001b[39mvocab_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m ])\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Step 4: Compile and train the model\u001b[39;00m\n\u001b[0;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Our small dataset (a list of sentences)\n",
    "sentences = [\n",
    "    'The cat sat on the mat',\n",
    "    'The dog ran in the park',\n",
    "    'The bird sang in the tree'\n",
    "]\n",
    "\n",
    "# Step 1: Convert words to numbers\n",
    "# We create a \"dictionary\" to assign a unique number to each word\n",
    "word_to_number = {}\n",
    "number = 1\n",
    "for sentence in sentences:\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_to_number:\n",
    "            word_to_number[word] = number\n",
    "            number += 1\n",
    "\n",
    "# Convert sentences to lists of numbers\n",
    "number_sequences = []\n",
    "for sentence in sentences:\n",
    "    sequence = [word_to_number[word] for word in sentence.lower().split()]\n",
    "    number_sequences.append(sequence)\n",
    "\n",
    "print(\"Sentences as numbers:\", number_sequences)\n",
    "\n",
    "# Step 2: Create training data\n",
    "vocab_size = len(word_to_number) + 1  # Total unique words (+1 for indexing)\n",
    "embedding_size = 10  # Size of word vectors\n",
    "window_size = 2  # Number of words to look at before and after the target word\n",
    "\n",
    "contexts = []  # Input: surrounding words\n",
    "targets = []   # Output: middle word\n",
    "\n",
    "for sequence in number_sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        # Context: words before and after the target (e.g., [word1, word2, word4, word5])\n",
    "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "        # Target: the middle word (e.g., word3)\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert to arrays for training\n",
    "X = np.array(contexts)  # Input data\n",
    "y = np.zeros((len(targets), vocab_size))  # Output data (one-hot encoded)\n",
    "for i, target in enumerate(targets):\n",
    "    y[i, target] = 1  # Set the target word's position to 1\n",
    "\n",
    "# Step 3: Build the model\n",
    "model = Sequential([\n",
    "    # Layer 1: Convert words to vectors (embeddings)\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2 * window_size),\n",
    "    # Layer 2: Average the vectors of context words\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    # Layer 3: Predict the target word\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 4: Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=0)  # Train for 100 rounds\n",
    "\n",
    "# Step 5: Get the word vectors\n",
    "embeddings = model.layers[0].get_weights()[0]  # Get the learned word vectors\n",
    "\n",
    "# Step 6: Reduce vectors to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(embeddings)\n",
    "\n",
    "# Step 7: Plot the words\n",
    "plt.figure(figsize=(6, 6))\n",
    "for word, idx in word_to_number.items():\n",
    "    x, y = reduced_vectors[idx]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.05, y, word, fontsize=10)\n",
    "plt.title(\"Word Vectors Visualized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdb6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
